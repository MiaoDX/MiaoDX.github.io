<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Customize hexo]]></title>
    <url>%2Fblogs%2Fhexo%2Fcustomize_hexo%2F</url>
    <content type="text"><![CDATA[Configurations of my hexo blog, now rendering with pandoc and automatic building/publishing with Travis-CI. Features Automatic build by Travis-CI Image/equation numbering and reference hexo-renderer-pandoc pandoc-fignos pandoc-eqnos Wiki-Style tooltip reference hexo-reference, seems not working Automatic build and publish by Travis-CI It has been one long-time wish to build and publish blog automatically, may dates from the very first time meeting hexo. See 1, 2, 3, and maybe many more online for references. For many of them, two branches/repository are used, one for hexo+blog source, the other for publishing. We divide the first one, and the system is like in Fig 1. The main advantage is that the source files are totally independent of hexo. Figure 1: The three-part hexo blogging system There are three orphan branches in this repository, source_files is all the blog source files and corresponding resources (images in most cases), hexo_template for the hexo configurations, and master for published blog HTML files. orphan branches serves it well to separate these branches without specific dependencies between each other, as in Fig 1, source_files and hexo_template are both equipped with Travis-ci, the former one just trigger the latter after new commits. So the majority of jobs are done at hexo_template CI: Install necessary libraries, hexo_cli, pandoc v2.x and pandoc filters pandoc-fignos, pandoc-fignos, etc NPM installation for package.json Get blog source files from source_file branch Get theme files &amp; override the theme configuration Duplicate resources/images folder by shell, this is due to the mechanism of hexo for serving images in the same-name folder of markdown files. hexo clean, hexo g Go to ./public directory, git init/add/commit, git push to master branch Done! As for the trigger procedure of source_files CI, there are two similar code snippets for this task, see stephanmg/travis-dependent-builds and plume-lib/trigger-travis. The former one only use Github Token (we already have it at the hexo_template pushing part) and login Travis to get the Access Token and do the trigger, the latter one requires Access Token as input, but have smaller processing time. See 4 for the differences between different types of tokens, and as shown in the bottom of it, we can get Access Token with Github Token with HTTP requests (which is neat since we may not have ruby environments at hand) as shown in Fig 2 and send the requests with postman as in Fig 3. That's it, we have set up the automatic updating blog system. Figure 2: Get Access Token with Github Token Figure 3: Get Access Token with Github Token Pandoc The default markdown rendering engine is hexo-renderer-marked, however, it performs poorly for images rendering, like specify the image width and numbering for reference, we choose hexo-renderer-pandoc instead. With pandoc filter pandoc-fignos, we can specify image width and refer to images like: 12345Please see figure @fig:answer_is_42 for more info.[...]![answer_is_42](pics/answer_is_42.png)&#123;#fig:answer_is_42 width=85%&#125; Pandoc version The hexo-renderer-pandoc changed its behavior at version 2.x, as discussed in 5, chances are that we can change some codes by ourselves to make it work, but we choose to install the newer version of pandoc with sudo dpkg -i Drawbacks &amp; workaround --- un-support When rendering files with many --- for dividing line, hexo failed in one way or another. So change --- to *** and done this forever :( TODO [ ] Compress the contents by gulp to make the rendering faster? https://segmentfault.com/a/1190000009054888↩ http://www.yanglangjing.com/2018/08/28/travis_ci_auto_deploy_hexo_to_vps/↩ https://notes.iissnan.com/2016/publishing-github-pages-with-travis-ci/↩ https://blog.travis-ci.com/2013-01-28-token-token-token↩ https://github.com/wzpan/hexo-renderer-pandoc/pull/22↩]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Camera Pose & Pose Estimation]]></title>
    <url>%2Fblogs%2Funrealcv_digest%2Fcamera_pose%2F</url>
    <content type="text"><![CDATA[Camera pose estimation is one of the most widely used low-level computer vision research, fundamentally supports SLAM, SfM, AR, VR and our ACR (Active Camera Relocalization). For one camera, we can express its pose by its location and rotation w.r.t. to one global coordinate, and for relative pose between two images, we can treat one as the global and assign \(I=[eye(3);0]\) to it. But this kind of intuitive description/assignment is not clear enough at all, especially when you try to code it (believe me -.-). For example, the location can be Camera in global coordinate or Global origin in camera coordinate, and same for rotation, Rotate from the global origin to the camera or to the origin, let alone these sentences like Recover relative camera rotation and translation from an estimated essential matrix and the corresponding points in two images (in OpenCV recoverPose function doc). Maybe you chose one of the four combinations and try to stick to it, but later found that it's not the choice your underlying library chose, which is rather frustrating. In this post, we try to specify one definition of POSE and its \(R,t\) parts and their physical meaning. Which are adopted by both OpenCV and OpenMVG (and maybe many others but not clearly stated), and with python code for demonstration. Camera Model Almost all literature use the very same pinhole camera model1,2, like in Fig . Pinhole Camera Pose and R, t Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the model coordinate system to the camera coordinate system. -- OpenCV solvePnP/solvePnPRansac doc. And, &gt; R: the rotation of the camera to the world frame, t: the translation of the camera. t is not the position of the camera. It is the position of the origin of the world coordinate system expressed in coordinates of the camera-centred coordinate system. The position, C, of the camera expressed in world coordinates is \(C=−R^{−1}t=−R^Tt\) (since R is a rotation matrix). --OpenMVG Camera Model So, both the documents are saying the same thing, the camera POSE will bring points in world coordinate into camera coordinate. a.k.a: \(Pose=[R|t]=[R|-RC]\) and \(p_{camera} = Pose * p_{world}\), And that's it, please read the OpenMVG statements again, the physical meanings of \(R, t\) and \(C\) are defined explicitly as stated. One major counterintuitive part is the \(R\), most of the time we rotate the camera from the origin by three Euler angle in one particular sequence, however, the \(R\) in pose means the rotation of the camera to the world frame, so, the inverse of our common rotating. Experiments We demonstrate with Pose3 module from OpenMVG[^mvg_pose3], by converting it into python[^acr_pose]. We set up one cube in a 3D world, change the location and rotation (pose) of the camera, project the cube into camera coordinate and 2D pixel. Showing that \(Pose, R, t, C\) do satisfy former statements. Then we do relative pose estimation with OpenCV's findHomography, findFundamentalMat, findEssentialMat, recoverPose, and solvePnPRansac and show that the API is pretty straightforward to use. 3D CUBE Rotation Demo, in camera pose the rotation means the rotation of the camera to the world frame Fig gives two example of camera pose change, we rotate the camera by predefined sequence (pitch, yaw then roll), but when calculating, use the inverse to make sure the rotation from camera to world frame. Codes are: We use 90° for visual convenience, for another example: if we rotate camera by \(R_{w2c}=(pitch=10°, yaw=20°, roll=30°)\), then \(R_{c2w}=R_{w2c}^T=(pitch=-19.01°, yaw=-11.82°, roll=-33.75°)\), which means we need to rotate the camera by completely another Euler angle (not just minus or so) to get the camera back. And of course, if we rotate by roll/yaw/pitch (the inverse) sequence, the angles are exactly minus, which should be easy to understand. 3D CUBE and projection Let's push it further by projecting more points and using them for camera pose estimation. Camera projection and way to get 3D points back with RGBD alike methods. findHomography, findFundamentalMat, findEssentialMat+recoverPose, solvePnPRansac are relative pose estimation methods provided by OpenCV. Relative pose estimation in OpenCV Method Input_A Input_B Constraints findHomography 2D pixel of 3D plane \(\Leftarrow\) \(s_i [p2;1] \sim H [p1;1]\), \(s_i\) is scale findFundamentalMat 2D pixel of 3D object \(\Leftarrow\) \([p2;1]^T F [p1;1]=0\) findEssentialMat 2D pixel of 3D object \(\Leftarrow\) \([p2;1]^T K^{-T} E K^{-1} [p1;1]=0\) recoverPose \(P_B \simeq RP*P_A\) \(\bar{t}\) is normalized, only has direction recoverPose 3D object 2D pixel of 3D object \(P_B = RP*P_A\) The take away is all APIs are constant, a.k.a: POSE= [R|t] = METHOD(A, B) will bring A to B -&gt; \(P_B = POSE * P_A\). Summary In this post, we try to specify the characteristics and physical meanings of POSE, R, t, C, which are widely used in low-level computer vision tasks. In conclusion: POSE: bring points in world coordinate into camera coordinate; R: the rotation of the camera to the world frame; t: the position of the origin of the world coordinate system expressed in coordinates of the camera-centred coordinate system; C: the camera expressed in world coordinates. And as one reminder, when we manipulating cameras, most of the time we say rotation from the point of view of the camera, like, in ACR, after calculating relative pose, we rotate the three Euler angle, which is okay. But in some scenarios, like the UnrealCV virtual environment, we are actually setting the camera rotation by API, such like vset /rotation/pitch=x,yaw=y,roll=z, that is, we can always and only rotate the camera from the world coordinate, no other way around. Say we want to capture images at pose of \([R|C]\), \(C\) is rather clear (set the location of the camera), but \(R\) in pose should be inversed so that we can rotate the camera from the origin. And after pose estimation, the very same procedure, \(R_{new}=Relative\_Pose * R\), inverse \(R_{new}\) for camera rotating. That's it. https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html↩ https://openmvg.readthedocs.io/en/latest/openMVG/cameras/cameras/↩]]></content>
      <categories>
        <category>Computer Vision</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Pose Estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Euler Angle, a program approach]]></title>
    <url>%2Fblogs%2Funrealcv_digest%2Feuler_angle%2F</url>
    <content type="text"><![CDATA[3D rotations matrices can make your head spin. I know it is a bad pun but truth can sometimes be very punny! -- https://www.learnopencv.com/rotation-matrix-to-euler-angles/ It has always been a burden to understand the physical meaning of Euler angle and do the rotation manipulation in code right, for a really long time. Each and every time I thought I was right and tweaked the codes and later found that I was wrong in one way or another. I will try to show how to do Euler angle right, both intuitively and with code. Hope this time I am right. Euler Sequence matters Rotations given as a set of three angles can refer to any of 24 different ways of applying these rotations, or equivalently, 24 conventions for rotation angles. 1 For example, in intrinsic scenario -- the axes move with the rotations like we rotate one dice. By only rotating around two axes, (X=90°, Y=90°) and (Y=90°, X=90°), we will get a totally different pose, as shown in . Rotate by XY and YX will bring to totally different pose, a.k.a the rotation sequence matters. Notations we use In most of our scenarios (camera relocalization workflow for the most time), we rotate cameras a lot, and intrinsic representation makes a great scene. There are some other representations of rotations like axis-angle and quaternion, but they are less intuitive in the hardware transformation view. And the rotation matrix is chosen for mathematical manipulating, which is also adopted by OpenCV in their pose estimation functions. UnrealCV as an example We use unrealcv2 extensively for our ACR (Active Camera Relocalization), and it alone is one interesting resource for many other computer vision projects. For rotating the camera as will, we need to find out the sequence of rotation in UnrealCV and do the transformation if necessary. We propose to rotate with a rather large angle to find out the rotation sequence in virtual env. E.g. rotate with (X=90°, Y=90°, Z=0), (X=90°, Y=0, Z=90°), (X=0, Y=90°, Z=90°) or (X=90°, Y=90°, Z=90°), with them, we can find the rotation sequence in UnrealCV is YXZ (yaw first, then pitch and roll). After we get the sequence, for demonstration, as in Fig , we transform (pitch(x)=10°, yaw(y)=20°, roll(z)=30°) in different order into YXZ, perform the rotation in UnrealCV and show the captured images as visual clue. Transform rotation in various sequences in certain one. So, we need to transform the rotation matrix (or one specific Euler sequence) into YXZ for image capturing. And the underlying mechanism stays the same when we choose to do camera rotation in other game engine or hardware. Conclusion Euler angle is one of the representations of 3D rotation and pretty intuitive one. However, the sequence of Euler angle makes a great difference, we need to figure out the right sequence before using it. And it makes no good to just GUESS and see how the result comes -- which we did a lot when implementing ACR at first. Codes designed carefully should be better than tweaked one. https://matthew-brett.github.io/transforms3d/reference/transforms3d.euler.html↩ https://github.com/unrealcv/unrealcv↩]]></content>
      <categories>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Euler Angle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The intrinsic of UnrealCV camera]]></title>
    <url>%2Fblogs%2Funrealcv_digest%2Funrealcv_K%2F</url>
    <content type="text"><![CDATA[The intrinsic of camera matters a lot in many low-level computer vision tasks, and in many scenarios maybe one plausive configuration is enough, there is no needs to get the exact value. In this post, we demonstrate how to get camera intrinsic quickly with an unavoidable error. Comments on github issue: The focal length f = W / 2 / tan(FOV/2) = W/2, where W is the width of the image which is 640 px here. And the pixels are square. https://github.com/unrealcv/unrealcv/issues/14#issuecomment-248554326 f = W/2 = 320px Get f with moving 4.2 Camera Calibration from Programming Computer Vision with Python We use a similar setup. We shot at one position, the move the camera right/up for certain pixel (\(x, y\)), measure the coordinate change of one certain pixel (\(d_x, d_y\)) and depth of camera \(d_Z\), the get \(f_x, f_y\). \[f_x = \frac{d_x}{x}d_Z\] In UnrealCV, the coordinate is X(front), Y(right), Z(up). With RealisticRendering, we take three images at (120, -120, 165), (120, -80, 165), (120, -120, 205) with FOV of \(90°\) and \(60°\). With the head of the right person on the boat, we can get corresponding coordinates: Corresponding coordinates with different FOV Camera Position/FOV 90 60 (120, -120, 165) (311, 197) (305, 165) (120, -80, 165) (146, 197) (19, 165) (120, -120, 205) (311, 362) (305, 451) Distance to wall, \(d_Z\) So, we need to get \(d_Z\), we can try to move the camera, and recognize that when the camera can not see anything, we are at the border, which is where Z (X in UnrealCV) = 198, then \(d_Z=198-120=78\). However, we can also get \(d_Z\) by moving the camera. We can move forward for certain distance \(U\), with \(\frac{X}{x_0}=\frac{d+U}{f}\) and \(\frac{X}{x_1}=\frac{d}{f}\). Corresponding coordinates with different FOV, the latter one of each item is the distance to the camera center. Camera Position/FOV 90 60 (120, -120, 165) (311, 197), (-9, -43)=43.93 (305, 165), (-15, -75)=76.49 (80, -120, 165) (314, 212), (-6, -28)=28.64 (310, 191), (-10, -49)=50.01 So, with FOV=90°, \(\frac{X}{28.64}=\frac{d+40}{f} and \frac{X}{43.93}=\frac{d}{f}\), we have d=74.92, with FOV=60°, \(\frac{X}{50.01}=\frac{d+40}{f} and \frac{X}{76.49}=\frac{d}{f}\), we have d=75.54. Then, \(d_Z=75\), the difference with \(78\) is not that big. Back to \(f\) For FOV=90°, \(d_x=311-146=165, x=120-80=40\), \(f_x =\frac{d_x}{x}d_Z=\frac{165}{40}78=321.75\). \(d_y=362-197=165, y=205-165=40\), \(f_y =\frac{d_y}{y}d_Z=\frac{165}{40}78=321.75\). For FOV=60°, \(d_x=305-19=286, x=120-80=40\), \(f_x =\frac{d_x}{x}d_Z=\frac{286}{40}78=557.7\). \(d_y=451-165=286, y=205-165=40\), \(f_y =\frac{d_y}{y}d_Z=\frac{286}{40}78=557.7\). Back to \(f = W/2/tan(FOV/2)\), with FOV=90°, \(f=W/2/tan(45°)=640/2/1=320\), with FOV=60°, \(f=W/2/tan(30°)=640/2/0.577=554.59\). So, the results are pretty nice. Camera intrinsics \[ K = \begin{bmatrix} f_x &amp; &amp; c_u \\ &amp; f_x &amp; c_v \\ &amp; &amp; 1\\ \end{bmatrix} = \begin{bmatrix} 320 &amp; &amp; 320 \\ &amp; 320 &amp; 240 \\ &amp; &amp; 1\\ \end{bmatrix} = \begin{bmatrix} width/2 &amp; &amp; width/2 \\ &amp; width/2 &amp; height/2 \\ &amp; &amp; 1\\ \end{bmatrix} \] That is it.]]></content>
      <categories>
        <category>Computer Vision</category>
        <category>Camera</category>
      </categories>
      <tags>
        <tag>Camera intrinsic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Have a glance at unrealcv]]></title>
    <url>%2Fblogs%2Fsynthesis%2Fhave_a_glance_at_unrealcv%2F</url>
    <content type="text"><![CDATA[This is the first post of my series of survey posts on synthesis ways to produce datasets for computer vision or virtual environment for DL/RL. Today we will have a look at a pretty neat tool -- unrealcv to acquire data from the Unreal4 world. In this post, you will discover: how to install unrealcv to your computer how to use the command interactively in RealisticRendering map how to use python code to fetch data Note: The official website is pretty nice organized and you're encouraged to read their document there or a richer official version. I record these things here just to make my series thorough and organized. Intro UnrealCV is a project to help computer vision researchers build virtual worlds using Unreal Engine 4 (UE4) -- Gitbub of unrealcv Features Before all, let us think what data we really need, for computer vision applications, we need: at least one camera the camera can be adjusted by providing the \(\mathbf{R}\) and \(\mathbf{t}\) the original image data, the normal, the mask, better if can get depth image the objects number, name, color The unrealcv meets all these common requirements and even more, see figure 1. Figure 1: vget /unrealcv/help Let us have a try! You are also encouraged to see official getting_started. Download the extended version of RealisticRendering for your platform Unzip and run the binary. Use the mouse to look around and use keys wasd to navigate, use qe to level the camera up and down, use ←↑→↓ to control the rotation of the camera. (Seem that ←→ do the yaw rotation well but ↑↓ is somewhat wired -- not pitch nor roll, but it's OK) Type ` (the key on top of the tab) to release the mouse and thus enter command line environment, type it twice to show enlarged command graphical interface. Type vget /camera/0/lit and navigate to the folder RealisticRendering\Binaries\Win64, we can see the 00000001.png we just captured. You should get something like figure 2 Type vget /unrealcv/help will get something like our first figure. TRY COMMANDS LISTED IN HELP Note: In Windows, chances are that you will encounter DirectX Runtime problem when trying to run the binary file, just download something like DirectX Repair V2.1 and repair the lacked files. Figure 2: 00000001.png Show me the code, please! We want to get tons of thousands of images from the virtual environment and then use them in various ways. So, we need codes to do that, and TES, we can do that! To use code fetch images via unrealcv is unreally easy:). See code snippet below (or just wget https://raw.githubusercontent.com/unrealcv/unrealcv/master/client/python/demo.py . --no-check-certificate): 12345678910111213from unrealcv import clientclient.connect() # Connect to the gameif not client.isconnected(): # Check if the connection is successfully established print 'UnrealCV server is not running. Run the game from http://unrealcv.github.io first.'else: filename = client.request('vget /camera/0/lit') print 'Image is saved to %s' % filename for gt_type in ['normal', 'object_mask']: filename = client.request('vget /camera/0/%s' % gt_type) print '%s is saved to %s' % (gt_type, filename) filename = client.request('vget /camera/0/depth depth.exr') print 'depth is saved to %s' % filename # Depth needs to be saved to HDR image to ensure numerical accuracy The code is pretty straightforward, we will get these files in a very short time. (Still in RealisticRendering\Binaries\Win64 folder) Note: The python version is version 2.x.x, and pip install unrealcv before typing python demo.py, and remember to launch the binary file first to start the game! Could it be better For me, It would be better if we can get: distance of the specific object from the camera or between objects (calculate from the depth image can lose accuracy (intuitive feeling) and not easy as a function call) Conclusion Today, we tried to install the unrealcv and use it both in the interactive mode and programming mode (python). What we did not cover is setting the camera position and rotation in the code, move the camera and fetch new images there, which is essential when getting images dataset. And, we are using the RealisticRendering virtual environment all the time, in practical usage, we need to create our own world and thus need to do some programming with Unreal Engine. Will talk about later (hopefully won't be too long). And you can see unrealcv/playground before my coming post covering that.]]></content>
      <categories>
        <category>Synthetic</category>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>synthesis</tag>
        <tag>unrealcv</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A survey of synthetic computer vision]]></title>
    <url>%2Fblogs%2Fsynthesis%2Fsurvey_of_synthetic_computer_vision%2F</url>
    <content type="text"><![CDATA[I am going to blog a series of survey posts on synthesis ways to produce datasets for computer vision or virtual environment for DL/RL, the idea first came into my mind when one of my upperclassman (FP, T) told me we can use data produced by simulation software for DL, and later on I realized that these synthesis ways are actually feasible when I encounter the works of openai, especially the gym and universe projects. And the survey starts when I was doing the project for Mrs. Liang Wan's Advanced Computer Graphics class. There are (at least) three main fields of the intended survey: Physics Simulations, like Bullet, Havok, MuJoCo, ODE, and PhysX, etc Game Engines, like Unity3D, Unreal, cocos2d-x, godot and so on Closed source games with (community) API for developers, like GTA V The first one can be part of the second one, and for synthesis images, the second and third can be more appealing since they provide abundant images of various scenarios. The potential difficulty is that the lacking of game development experience, especially in the second and third one. But the promising outcome is really appealing and maybe not so much experience in game development is needed at all. What's more, all of these three have successful prior work, the openai gym uses MuJoCo in their project, and there is unrealcv which aim to help computer vision researchers build virtual worlds using Unreal Engine 4, then there is one article Using Virtual Worlds, Specifically GTA5, to Learn Distance to Stop Signs. So I am going to survey these three in the first round. To make things easy for referring and organized, all blogs are listed below with logical order: Have a glance at unrealcv]]></content>
      <categories>
        <category>Synthetic</category>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Story of Configuration]]></title>
    <url>%2Fblogs%2Fconfiguration%2FSeriesOfConfiguration%2F</url>
    <content type="text"><![CDATA[It all starts when I was very very young, my mother bought me rattle-drum and when I got home I immediately cracked open it to see how it actually make a sound. Or even before that, when I use a screwdriver to open the electric sewing machine and got an electric shock. Sometimes I even wonder maybe it's better for me to study engineer instead of a software engineer in college. -.- From spending days to download visual studio 2012 in low speed Internet connection in library when I was freshman to making self-build host on DigitalOcean, from applying for SuperVessel to hosting class project on Aliyun, from building OpenCV2 and OpenCV3 with python2 and python3 on windows and recently taking some like two weeks to build a program. The amount of time spending on these configurations really is countless. A pretty shamed thing to admit is that most of my configurations lead to nothing, after days and nights' hardwork, I just never look at these things after finishing configuring them. So, why is this blog? I just want to remind myself that: Time you enjoy wasting is not wasted -- Marthe Troly-Curtin Ref Even sometimes I really dislike the despair when tried once and once again, things just won't behave as wish. And from today on, I will try to record some of these kinds of really time-consuming configuration stuff and try to do a Lesson Learned at the end of these writeups and in this blog. These won't necessarily be a blog, maybe just some files in some Github repositories. Hopefully, this configuration for configuration can really last. Building angelix. Angleix is a semantics-based automated program repair tool for C programs, I choose to ingest it for my project of graduate software testing class. This program uses lots of other projects to perform backend computing jobs, such as klee. The problem is that it uses some codebases of the master branch and these codes finally cause dependences problems and made my build failed in many ways. So, if we are going to publish codebase with using lots of other modules, just FREEZE the versions like in many code management tools. This can be hard when multiple languages are involved, like in angelix, but we should try our best to make things easier for both our potential users and our future self.]]></content>
      <categories>
        <category>Configuration</category>
      </categories>
      <tags>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell Scripts]]></title>
    <url>%2Fblogs%2Ftips%2FShell_Scripts%2F</url>
    <content type="text"><![CDATA[Some handy shell scripts useful for myself and maybe others. Nowadays I am using windows a lot, the shell shipped with windows is not so appealing, instead I use Gow, however, it's still not the same as *ux systems. So some commands are used in the MINGW64 window come along with Git. :blush: for both Gow and Git shell :penguin: for Git shell but not Gow :scream_cat: for Gow but not Git shell (since it really amazing, can shell in Linux like worse than windows?) iconv :penguin: iconv: Converts text from one encoding to another encoding. iconv -f GBK -t UTF-8 gbk_file.txt &gt; utf_8_file.txt To convert back, please plus the -c configuration to discard unconvertible characters iconv -c -f UTF-8 -t GBK utf_8_file.txt &gt; gbk_file.txt However, it would be better if we can change all files (maybe one specific extension) in a directory to certain encoding. Here you are: 1find src_dir/ -name &quot;*.txt&quot; -type f -exec ./change_encoding.sh &#123;&#125; \; see change_encoding.sh for more info.]]></content>
      <categories>
        <category>Tips</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Some tools]]></title>
    <url>%2Fblogs%2Ftips%2FSome_tools%2F</url>
    <content type="text"><![CDATA[Why and what There are lots of nice or great tools on the internet, sometimes I will just forget a possible Silver bullet for a particular situation. So this blog is just an entrance to a nice or clean or amazing world just by small tools. Windows Package manager Once tried command line, it makes you think agree with the agreements -&gt; choose where to install -&gt; next -&gt; next unbearable. Thanks give to Nodejs chocolatey choco install WHAT_YOU_WANT Or scoop scoop install WHAT_YOU_WANT I find scoop latter, but I think it may be better. A small shell Gow Can be installed via choco or scoop. Dependence Walker Dependence Walker: Dependency Walker is also very useful for troubleshooting system errors related to loading and executing modules. Dependency Walker detects many common application problems such as missing modules, invalid modules, import/export mismatches, circular dependency errors, mismatched machine types of modules, and module initialization failures.]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to install opencv in WINDOWS]]></title>
    <url>%2Fblogs%2Fopencv%2FHow_to_install_opencv_in_WINDOWS%2F</url>
    <content type="text"><![CDATA[在 windows 上编译安装 opencv2 以及 opencv3，同时配置 python2 与 python3。预期目标是能够同时使用 python2/3 与 opencv2/3，共三种组合方式（opencv2 不支持 python3.x 版本）。 Linux，macOS，Raspberry Pi 等其他平台上的 OpenCV 安装请在 OpenCV 3 Tutorials, Resources, and Guides 寻找合适的方法，本文很大程度上参考了系列中的 Ubuntu 16.04: How to install OpenCV，甚至可以说是其在 windows 上的移植。 Step 0.Goal 这里假设我们想要把 opencv 安装到 C:\tools\opencv_all_in_one，也就是分别是 C:\tools\opencv_all_in_one\opencv2 与 C:\tools\opencv_all_in_one\opencv3，最终希望与直接从官网下载解压下来的内容相似，就像是： 且同时支持 python2 与 python3。 Step 1.Requirements Win10 [win7 应该也可以，没有做测试] VS2015 community [Free 的 community 版本，Long live Microsoft -.-] cmake git python2 and/or python3 Note：安装后三项时建议使用 choco 安装，可以自动添加 PATH，另外使用命令行会减少不少截图工作。 下面命令，如无特别说明建议在 git 提供的命令行窗口 -- 左上角有 MINGW64 的那个窗口执行： git_shell 123456789101112# 确保所需软件$ pwd# c/tools$ which python# /c/Python36/python$ which cmake# /c/Program Files/CMake/bin/cmake$ which git# /mingw64/bin/git 另外，我的 python2 安装在了 C:\tools\python2 中（choco 自动选择了此目录），请确保 python2/3 均安装了 numpy（pip install numpy）。 Step 2.Get opencv from github 123456$ pwd# /c/tools$ git clone https://github.com/opencv/opencv.git opencv_git# [...]$ git clone https://github.com/opencv/opencv_contrib.git opencv_contrib# [...] 其中 opencv_contrib 是因为 opencv3 中把许多 Non-free 模块放在了里面（比如著名的 SIFT,SURF 等），另外还有一些目前并不太成熟的新算法也都在里面，我们在编译 opencv3 时需要使用它。 Step 3. 编译 opencv2 3.1. 切换到合适的版本 查看可以选择的版本，在输入 git checkout 后按 TAB 建： 1234567891011$ pwd# /c/tools$ cd opencv_git$ git checkoutDisplay all 58 possibilities? (y or n)2.2 2.4.10 2.4.12 2.4.2 2.4.4-beta 2.4.6.2r3 2.4.8 3.0.0 3.1.0 origin/2.42.3.0 2.4.10.1 2.4.12.1 2.4.3 2.4.5 2.4.6.2-rc1 2.4.8.1 3.0.0-alpha 3.2.0 origin/HEAD2.3.1 2.4.10.2 2.4.12.2 2.4.3.1 2.4.6 2.4.7 2.4.8.2 3.0.0-beta 3.2.0-rc origin/master2.4 2.4.10.3 2.4.12.3 2.4.3.2 2.4.6.1 2.4.7.1 2.4.8.3 3.0.0-rc1 HEAD tags/2.4.13.22.4.0 2.4.10.4 2.4.13 2.4.3-rc 2.4.6.2 2.4.7.2 2.4.9 3.0-ocl-tech-preview heads/2.4.13.22.4.1 2.4.11 2.4.13.1 2.4.4 2.4.6.2r2 2.4.7-rc1 2.4.9.1 3.0-ocl-tp2 master 1234567# 选择 2.4.x 中最新的版本$ pwd# /c/tools/opencv_git$ git checkout 2.4.13Checking out files: 100% (5476/5476), done.Previous HEAD position was 70bbf17... OpenCV Version++HEAD is now at 59975db... Merge pull request #6441 from asmorkalov:version++ 3.2. 进行编译配置 我们上面的 opencv_git 文件夹仅用于存储官方的文件，下面新建一个临时文件进行编译： 123456$ pwd# /c/tools$ mkdir opencv_build_v2_tmp # 新建用于编译 opencv2 的文件夹$ cd opencv_build_v2_tmp/$ cmake-gui # 打开 cmake-gui 界面 将 opencv_git\CMakeLists.txt 拖入界面: Note:我的截图中 opencv 是在 opencv3_git 文件夹下，请原谅 1.更改目标路径 2.点击 Configure 3.确保生成器合适 4.点击 Finish make_1 会自动生成配置文件，另打开一个 git-shell： 1234567891011$ pwd# opencv_build_v2_tmp/$ ls3rdparty/ cvconfig.h OpenCVConfig.cmakeapps/ data/ OpenCVConfig-version.cmakecmake_uninstall.cmake doc/ unix-install/CMakeCache.txt include/ version_string.tmpCMakeFiles/ junk/ win-install/CPackConfig.cmake modules/CPackSourceConfig.cmake opencv2/ 此时的截图如下： make_2 在 Search 框搜索 python 确保 python 被正确找到 也可在下面的输出结果中查找 python 项 然后，点击 Generate，会发现下面的输出框最后一行出现： 1Generating done 3.3. 编译 之后，我们就可以编译了，（双击）打开 opencv_build_v2_tmp 中的 OpenCV.sln，是一个熟悉的 VS 工程： make_3 其中： 1.CMakeTargets 保持默认 ALL_BUILD 即好 2.选择 Release 与 x64 版本 3.点击 x64 右侧的下拉框 4.出现的窗口中第三项 INSTALL 的 Build 默认是没有勾选的，将其勾选 之后，等一段时间（编译阶段 CPU 占用会很高） 编译结果为： make_4 不必在意提示的错误，这是因为我们编译后并没有可运行的文件 1.输出框最后指示出全部编译成功！！ 2.cv2.lib 即为 python2.x 需要的 上面也可以选择不进行 INSTALL，在编译结束后可以另外安装，此时可以将 CMakeTargets 中的 INSTALL 右键设置为 Set as StartUp Project，运行，瞬间结束，输出很好地指示我们安装在了哪里： make_5 3.4. 整合编译结果 此时，C:\tools\opencv_build_v2_tmp\bin\Release 包含了一些例子的可执行文件，还有许多的 dll 文件，C:\tools\opencv_build_v2_tmp\lib\Release 中包含了我们需要的库。 但，我们几乎只需要 C:\tools\opencv_build_v2_tmp\install 中的文件即可： opencv2_build_compare 与官方提供版本相比，少了一些东西，需要注意的是我们前面说到的 python 使用，这里并没有，但编译结束时确实生成了，所以我们自己将其取出来： 新建 python\2.7\x64 的三层文件夹，将 C:\tools\opencv_build_v2_tmp\lib\Release\cv2.pyd 复制到 C:\tools\opencv_build_v2_tmp\install\python\2.7\x64 中。 3.5. 配置编译环境 复制需要的文件 12345678910$ pwd# /c/tools$ mkdir opencv_all_in_one$ cd opencv_all_in_one/$ mkdir opencv2$ cd opencv2# 将 install 文件夹下所有文件复制到 opencv2 文件夹下$ cp c:/tools/opencv_build_v2_tmp/install/* -rf . 将 install 文件夹下所有文件复制到 opencv2 文件夹下，此时我们可以直接删除 c:/tools/opencv_build_v2_tmp 整个文件夹了，我们只需要 C:\tools\opencv_all_in_one\opencv2 文件夹内的内容。 配置 PATH 路径 这里参考 CMake 简介和 CMake 模板 来验证，其中 OpenCV 的配置可以参考同一作者写的教程： 123456789# copy and change from https://github.com/district10/cmake-templates/issues/4set these two proper environment variables:* OpenCV2_DIR -&gt; C:\tools\opencv_all_in_one\opencv2* OpenCV3_DIR -&gt; C:\tools\opencv_all_in_one\opencv3 # 此文件夹我们待会儿编译 opencv3 时会生成* OpenCV_DIR -&gt; %OpenCV2_DIR% or %OpenCV3_DIR% (which you prefer)add to PATH:* %OpenCV2_DIR%\x64\vc10\bin * %OpenCV3_DIR%\x64\vc14\bin Update 20170410: 根据需要更改 vc10 或者是 vc14。 3.6. 验证编译、配置无误 在使用原作者提供的 CmakeLists 时请确保使用 cmake-gui 来保证我们是生成合适的目标，即： build 确保是生成 Visual Studio 14 2015 Win64 的工程（这也是刚我们编译 OpenCV 时选用的配置）。 直接使用 cmake 很可能会出错： 12345$ pwd# example_opencv$ mkdir build$ cd build$ cmake .. 会得到： 12345678910111213[...]-- OpenCV ARCH: x86-- OpenCV RUNTIME: vc14-- OpenCV STATIC: OFFCMake Warning at C:/tools/opencv_all_in_one/opencv2/OpenCVConfig.cmake:163 (message): Found OpenCV Windows Pack but it has not binaries compatible with your configuration. You should manually point CMake variable OpenCV_DIR to your build of OpenCV library.Call Stack (most recent call first): CMakeLists.txt:11 (include)[...] 很明显，它默认选择的 OpenCV ARCH 是 x86，而我们需要的是 x64，所以，为减少不必要的麻烦请使用 cmake-gui。 Step 4. OpenCV3 的编译 编译 OpenCV3 与上面的步骤很相似，不过要将之前提到的 opencv_contrib 模块用上。 4.1. 选取合适的版本 12345678910111213141516171819202122$ pwd# /c/tools# 选取合适的 opencv 版本$ cd opencv_git$ git checkout 3.2.0# Checking out files: 100% (5476/5476), done.# Previous HEAD position was 59975db... Merge pull request #6441 from asmorkalov:version++# HEAD is now at 70bbf17... OpenCV Version++# 选取对应的 contrib 版本$ cd ../opencv_contrib/$ git checkout 3.2.0# HEAD is now at 8634252... Merge pull request #701 from LaurentBerger:DericheFilter$ cd ..$ pwd# /c/tools$ mkdir opencv_build_v3_tmp$ cd opencv_build_v3_tmp$ cmake-gui . 请确保 contrib 模块与 opencv3 的版本相同，不然很可能出现一些问题。 4.2. 验证 python 版本 首先确保 python 版本没有问题，在 Search 框内搜索 python 会得到类似于下面的结果： make_v3_1 可以看出自动检测到了 python2 与 python3 的路径，这样便会生成两者的 binding。 4.3. 添加 contrib 库 此时需要将 contrib 加入进来，在 Search 框内输入 extra，将对应路径填入右侧空格，需要注意的是填入的是 C:/tools/opencv_contrib/modules 而不是 C:/tools/opencv_contrib （前者多了 modules 目录）： make_v3_2 4.4. 整理编译得到的内容 之后便是编译了，与上面没有什么不同，这里看一下最终得到的 install 文件夹内的内容： make_v3_3 还是没有我们想要的 python 文件夹，与上面类似，新建 python\2.7\x64 的三层文件夹，将 C:\tools\opencv_build_v3_tmp\lib\Release\cv2.pyd 复制到 C:\tools\opencv_build_v3_tmp\install\python\2.7\x64 中，类外，我们一直说的 python3 支持还没有做到，会发现 C:\tools\opencv_build_v3_tmp\lib\python3\Release 文件夹内有一 cv2.cp36-win_amd64.pyd 文件，这便是 python3 对应的 cv2.pyd，将其复制到新建的 C:\tools\opencv_build_v3_tmp\install\python\3.6\x64 文件夹下。 将整个 C:\tools\opencv_build_v3_tmp\install 文件夹内容复制到 C:\tools\opencv_all_in_one\opencv3 中。 好，算是结束了。 4.5. 验证安装 验证方法与上面 opencv2 时相同，不再赘述。 Step 5. python binding 我们上面均进行了 python binding 的编译，也将文件放在了合适的地方，但是还未曾进行验证。 在 INSTALL 工程中已经将 python binding 安装到了所选择的 python2/3 中： 123456$ pwd# C:\tools\opencv_build_v3_tmp$ cat install_manifest.txt | grep pyd# C:/tools/python2/Lib/site-packages/cv2.pyd# C:/Python36/Lib/site-packages/cv2.cp36-win_amd64.pyd 其中 install_manifest.txt 记录了安装到的路径，最简单的检验方法便是直接试一下： 12345678$ python# Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import cv2&gt;&gt;&gt; cv2.__version__# &apos;3.2.0&apos;&gt;&gt;&gt; sift = cv2.xfeatures2d.SIFT_create() 或者是： 1234567$ C:\tools\python2\python# Python 2.7.11 (v2.7.11:6d1b6a68f775, Dec 5 2015, 20:40:30) [MSC v.1500 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import cv2&gt;&gt;&gt; cv2.__version__# &apos;3.2.0&apos; 5.1. virtualenv 配置工作环境 但是安装到全局也许并不是我们最希望的，比如说此时 opencv2+python2 的组合是没有的（因为是后来编译的 opencv3，将之前的结果覆盖了），所以建议使用 virtualenv 来建立三个虚拟 python 环境： 12345678$ pwd# /h/py_env # 可以自己选择合适的位置$ virtualenv --always-copy -p C:/tools/python2/python.exe cv2_py2# New python executable in H:\py_env\cv2_py2\Scripts\python.exe# Installing setuptools, pip, wheel...done.# Running virtualenv with interpreter C:/tools/python2/python.exe 其中，--always-copy 指 Always copy files rather than symlinking，建议如此，但是 -p C:/tools/python2/python.exe 是必要的，不然会生成默认版本的 python（这里指 python3）。 下面的命令请在 cmd 中运行（在 git-shell 中我是遇到了一些问题） 12345$ pwd# H:\py_envH:\py_env&gt;cv2_py2\Scripts\activate(cv2_py2) H:\py_env&gt;which python# H:\py_env\cv2_py2\Scripts\python.EXE 安装 numpy，并将合适的 cv2.pyd 复制到 site-packages 中： 123(cv2_py2) H:\py_env&gt;pip install numpy[...](cv2_py2) H:\py_env&gt;cp C:\tools\opencv_all_in_one\opencv2\python\2.7\x64\cv2.pyd cv2_py2\Lib\site-packages 验证： 12345678(cv2_py2) H:\py_env&gt;pythonPython 2.7.11 (v2.7.11:6d1b6a68f775, Dec 5 2015, 20:40:30) [MSC v.1500 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import numpy&gt;&gt;&gt; import cv2&gt;&gt;&gt; cv2.__version__# &apos;2.4.13&apos;&gt;&gt;&gt; detector = cv2.FeatureDetector_create(&quot;SIFT&quot;) 请注意与上边调用 SIFT 的区别，sift = cv2.xfeatures2d.SIFT_create()(opencv3) 与 cv2.FeatureDetector_create("SIFT")(opencv2)，这也证实了我们也成功编译了 contrib 库。 cv3_py2 与 cv3_py3 虚拟环境的安装与配置之无差，不再赘述 可能有一点需要注意的是 opencv3 生成的 python3 对应的文件为 cv2.cp36-win_amd64.pyd，不必太在意，将其当作 cv2.pyd 即可（也可以将其重命名为 cv2.pyd，但不必要）： 1(cv3_py3) H:\py_env&gt;cp C:\tools\opencv_all_in_one\opencv3\python\3.6\x64\cv2.cp36-win_amd64.pyd cv3_py3\Lib\site-packages 总结 我们在 windows(10) 上源码编译了 opencv2 与 opencv3，同时也生成了对应的 python binding，且使用 virtualenv 来配置干净的编程环境。 我们没有涉及的 需要注意到我们并没有将 opencv 的性能发挥到很好，有许多加速功能我们并没有配置： GPU/TBB/OpenGL support Matlab/Java binding 想要了解一部分相关内容，请参见官方文档：Installation in Windows¶ UPDATE 20170410，CUDA 支持 最近尝试了网上 GPU(CUDA) 版本的 SIFT，同时编译了一下 opencv 的 GPU 支持，遇见了一点问题。 Win10 VS2015 community cmake CUDA v8.0 1234567891011121314C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\demo_suite&gt;deviceQuery.exedeviceQuery.exe Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: &quot;GeForce GT 530&quot; CUDA Driver Version / Runtime Version 8.0 / 8.0 CUDA Capability Major/Minor version number: 2.1 Total amount of global memory: 1024 MBytes (1073741824 bytes) ( 2) Multiprocessors, ( 48) CUDA Cores/MP: 96 CUDA Cores GPU Max Clock rate: 1399 MHz (1.40 GHz) [...] 遇到了类似于： 12C2610 and C2535 error for file tuple in the opencv_perf_cudawarping project.C2382 error for file cuda_perf.hpp in the opencv_perf_cudawarping project. 的错误，在 OpenCV 的 issue 下找到了一个 workaround: 1What I did was to enable WITH_CUBLAS aswell as WITH_CUDA. I also turned off BUILD_PERF_TESTS and BUILD_TESTS. 这被标记成了一个 bug，所以应该会被修复，放在这里是为了提醒大家如果遇到了编译错误，可以考虑首先去 opencv 的 github issue 中寻找解决方案。]]></content>
      <categories>
        <category>Configuration</category>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nodejs debug tips]]></title>
    <url>%2Fblogs%2Fnodejs%2Fnodejs_debug_tips%2F</url>
    <content type="text"><![CDATA[What &amp; why 写了一些 nodejs 代码之后，深感调试很不方便。百度以及 SO 了一段时间（额，一个多小时）发现了一个很不错的解决方案： 需要 node 版本 7.×.×： node --inspect --debug-brk app.js 使用 node-supervisor 来监视你对代码的改动，并自动重启 Node。 对应到 npm 的 package.json 中可以有如下的设置： 1234"scripts": &#123; "start": "supervisor ./bin/www", "test": "supervisor --inspect --debug-brk ./bin/www"&#125;, 另外，Chrome DevTools: Live edit running Node.js code with hotswapping 原本看着很炫酷、很实用，可是在我更新了最新的 chrome 没有发现那里有这个功能。]]></content>
      <categories>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use mongo in docker and dump database]]></title>
    <url>%2Fblogs%2Fnodejs%2Fuse_mongo_in_docker_and_dump_database%2F</url>
    <content type="text"><![CDATA[使用 mongo docker 镜像 使用 mongo 镜像是很方便的，直接使用官方镜像就好了，为了今后更方便使用，这里给出依据 restheart-docker 中的 docker-compose.yml 裁剪出一个还不错的方案（也就是剪除 restheart 部分而已）： 12345678910111213141516171819202122232425# vi docker-compose.ymlversion: &apos;2&apos;### Creates a named network with the default bridge drivernetworks: backend:### Creates a permanent, named data volume# This makes much easier to identify where the mongodb data is stored on host# See: https://docs.docker.com/engine/userguide/containers/dockervolumes/#mount-a-shared-storage-volume-as-a-data-volumevolumes: datavolume:services: mongodb: image: mongo:3.2 ### Uncomment to name the container explicitly # container_name: your-mongo-name volumes: - datavolume:/data/db networks: - backend ports: - &quot;27017:27017&quot; 运行 docker-compose -f docker-compose.yml up 后便会成功启动镜像了。上面需要解释的有两个地方，networks 与 volumes 会在第一次启动后新建一个网络与卷，名字分别为 &lt;nowpath&gt;_backend 与 &lt;nowpath&gt;_datavolume，假设当前路径为 testmongo，则有如下输出（第一次）： 123456Creating network &quot;testmongo_backend&quot; with the default driverCreating volume &quot;testmongo_datavolume&quot; with default driverCreating testmongo_mongodb_1Attaching to testmongo_mongodb_1mongodb_1 | 2016-12-11T13:15:09.269+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=4929d787dd14[...] 此时可以查看网络与 volume 信息：docker network inspect testmongo_backend 与 docker volume inspect testmongo_datavolume，特别是 volume inspect 可以看到存储的挂载点： 123456789[ &#123; &quot;Name&quot;: &quot;testmongo_datavolume&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/testmongo_datavolume/_data&quot;, &quot;Labels&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;] 另外，container_name 是可配的，在实际使用时最好是进行配置。 数据库导出 上面得到的挂载点是存储着 mongo 信息的，但是当我们想到导出当前数据库时这些数据是不可用的（格式不对）。 预计需要用到的指令：MongoDB 备份(mongodump)与恢复(mongorestore)，mongodump，mongoexport。 在 docker-compose up 后，我们在数据表中添加一行记录： 在本地（Linux 主机或虚拟机）安装了 mongo （客户端）之后，运行 mongo 命令： 12345MongoDB shell version: 2.6.10connecting to: testServer has startup warnings:[...]&gt; 12345678910# 新的数据，参考 http://www.runoob.com/mongodb/mongodb-insert.html&gt; use test&gt; db.col.insert(&#123;title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;) 数据表/集合（collection）导出可以直接使用 mongoexport 来导出：mongoexport --db test --collection users --out users.json 但是对一个数据库的导出是不可用的，mongodump --db test --out . 会在当前路径下生成一个空的 test 文件夹，而事实上是有一条数据的。这是一个需要解决的问题，在 SO 上有类似的解决方案：docker-machine ssh command for mongodump，答案中也有我的一些注释，整理一下如下： 想法是新建一个与挂载 volume（这里最好是一个本机路径，容易获取结果）的 mongo container，与当前正在运行的 mongo container 进行连接，执行 dump 指令： 12345678# vi mongodumpFromDocker.sh# A nice method to mongodump from Docker containersudo docker run \ --net testmongo_backend \ --link testmongo_mongodb_1:mongo \ -v $(pwd):/backup mongo:3.2 bash \ -c &apos;mongodump --out /backup --host $MONGO_PORT_27017_TCP_ADDR&apos; 12$ chmod +x mongodumpFromDocker.sh$ ./mongodumpFromDocker.sh 这会当前路径下生成正确的包含数据与数据库信息的 test 文件夹。上面的 bash 文件中 testmongo_backend 是我们上面生成的网络，testmongo_mongodb_1 是因为我们没有指定 container 的名称，$MONGO_PORT_27017_TCP_ADDR 需要换为一个真实的 IP 地址，0.0.0.0 是不可用的，以我跑在 Virtualbox 中的虚拟机为例，最后一行可以改为： -c 'mongodump --out /backup --host 100.100.100.10' 或 -c 'mongodump --out /backup --host 100.100.100.10:27017' 数据库导入 mongorestore 可以很好的工作： mongorestore -d test2 test 会在 mongo container 中新建一个与 test 文件夹包含的数据信息相同的数据表（collection） test2。 总结 1.上面的用法并非是 production ready 的，特别是没有设置用户名与密码 2.mongo 的几个数据库/表迁移命令只有 mongodump 对 docker 不透明，感觉今后也许会有更新，毕竟我们是可以直接查询到数据内容的，无法一键备份是有些没有道理。 Good Luck &amp; Have Fun!]]></content>
      <categories>
        <category>Configuration</category>
        <category>Database</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mongodb</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debug python with docker in pycharm]]></title>
    <url>%2Fblogs%2Fdocker%2FDebug_python_with_docker_in_pycharm%2F</url>
    <content type="text"><![CDATA[Why As I really really appreciate it that we can have an isolated development environment, when I heard pycharm can debug with docker, I was more than happy. However, things are not that easy. What I got I tried, tried and tried. At an amazing moment, I succeeded running docker on windows through DockerToolbox. Mostly, I refer One answer to Running a Docker image in PyCharm causes ¡°Invalid volume specification¡±. (By the way, my machine and environment is exactly like the answer) BUT suddenly I failed to repeat it. So I changed to try it on ubuntu Unbuntu 14.04 in Virtualbox. The default settings when adding docker as shown in fig 1 is not suitable for Ubuntu 14. Figure 1: add_docker_linux screenshot According to another SO question's answer, adding unix:///var/run/docker.sock to the API URL shows Connection successful when click Apply. But when running programs it still won't work. And then Problem partially solved, when we use Docker to debug our project through PyCharm, it's not attach to a running docker and/or exec the commands (by now). Instead, it starts a new Containner and runs the commands, so if we want to debug a python file, we should make sure the docker running python when open. In other words, the Dockerfile better with the last sentence: 1CMD ["python3"] And in Linux (Ubuntu) when this assured, all things are OK. And now I can debug with alpine-python TODO After succeeding in Linux, I go back and try in windows, it's surely a bug of Pycharm and hope can be solved in a later version. And what's more, what I really want is more than this, I hope as below: 123456789+------------------------------+| | +------------------+ +-------------------+| | &lt;----------------+ | | || Docker/rkt/others | | Desktop UI XXXXXX Happy ME || +----------------&gt; | | | || | +------------------+ +-------------------++------------------------------+ Some remote servers In other words, no applications on PC/Desktop, no more reinstall systems, no different configurations, no more configurations problems! But with full control over what we use. I will review related techs in the future, and this is another weekend I spent play with docker and all the configurations and DIT NOTHING WORTHFUL. Well, that's typical me. -.-]]></content>
      <categories>
        <category>Configuration</category>
      </categories>
      <tags>
        <tag>debug</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java MVC environment backend]]></title>
    <url>%2Fblogs%2Fjava%2Fjava_mvc_environment_backend%2F</url>
    <content type="text"><![CDATA[项目环境搭建 -- 后端 主要内容：Intellij+SpringMVC+Docker_tomcat+Jenkins_CI === update 20160915，今天一晚上耗在了这上面，原本是在新的台式机上插件安装明显是有问题（插件查询页面全都没有反应），后来在虚拟机上安装 docker 镜像直接显示“This Jenkins instance appears to be offline. ”，以为是虚拟机联网有问题（但 ping 百度是正常的），在 aliyun 的主机上安装也是同样的错误，然后在本地卸载后重新安装竟然也显示 offline 了。然后才惊觉也许是联网有问题？？打开 vpn 后发现真的是可以安装插件了，这种问题耗费时间确实不该啊。（上次在笔记本上配置时很正常，也许是当初开着 vpn 的，要么就是这一段时间之内 GFW 把 Jenkins 的插件网站给封了？？） 另按照建议安装插件后，请单独再安装 “Deploy to container Plugin” 以及 “Maven Integration plugin”。 声明 这节博客有些“碎碎念”，主要是今晚不准备写新的程序，如果想要节省时间，请直接参考下面的三/四篇文章： 使用IntelliJ IDEA开发SpringMVC网站（一）开发环境 Docker自动部署Apache Tomcat，文末有原文链接。 jenkins+maven+svn构建项目，及远程部署war包到tomcat上 比第三篇更详细一些的： 「Jenkins+Git+Maven+Shell+Tomcat持续集成」经典教程 非常感谢原作者们。 缘由 本项目（可看做原要求的一种形式，即以小组形式完成），简单地说，以小组方式完成一个“小学生四则运算程序”，题目来源于邹欣的构建之法 中 1.1 节程序员阿超与学校老师的故事（莫名地想到了渔夫与金鱼），使用 Java Web 来做乍看起来可以说是有些“太重了”，但要做到整个小组都参与进来，并没有其他特别好的方法（如果有的话，请告诉我 :&gt;）。 当然，因为需求有变化的可能，所以使用较为成熟的技术还是有更多保障的。 另外，组员们都没有正式做过类似的项目，而今年另一门课程“软件开发基础”也会讲到 Java 开发（Design Patterns 等），可以说使用 Java 开发算是一举多得。 如此可以将整个项目分为前端与后端两部分，本文记录一下后端的环境搭建。 真正的缘由 本科学 Java 时因为受到类似于 Java “执行慢”、“笨重”、“有重大设计缺陷”等等说法的误导以及在课上没有跟上老师飞快的讲授节奏还有自己的懒惰，所以学得很不好。可是后来发现其实现实中 Java 能做的东西是非常多的，所谓执行效率，就目前的硬件来说，除非在特定领域（图像、视频处理，硬件控制，高时序要求等）并不会那么明显。其他的诸多好处就不再赘述。 开发环境 Intellij 是目前最好的 Java 开发环境（如有异议，请把它当作“PHP 是最好的编程语言”） Spring： The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. 算是目前业界正在广泛使用的框架，当然，还是应该有数据持久化框架的，不过这个先不介绍（Hibernate 与 Mybatis 还没确定使用哪个）。 请移步 使用 IntelliJ IDEA 开发 SpringMVC 网站（一）开发环境，建议一步步看下去，非常好。 ... ... ... &gt;&gt;&gt; &lt;&lt;&lt; &gt;&gt;&gt; 大概需要半个多小时对应着搞完。 其实开发环境的配置就结束了 ~ 当然，在后面你可能需要一台机器上启动多个 Tomcat，由于原博客有迁移的迹象，现复制如下。 如果需要在一台机子上启动多个Tomcat服务器，在默认设置下肯定会发生端口冲突。为实现这个效果，只需修改conf子目录中的server.xml文件即可。共需修改三处： 12345678(1).修改http访问端口（默认为8080端口）:&lt;Connector port=”8080” protocol=”HTTP/1.1″ connectionTimeout=”20000″ redirectPort=”8443″ URIEncoding=”gb2312″/&gt;(2).修改Shutdown端口（默认为8005端口）:&lt;Server port=”8005” shutdown=”SHUTDOWN”&gt;(3).修改JVM启动端口（默认为8009端口）:&lt;Connector port=”8009” protocol=”AJP/1.3″ redirectPort=”8443″ /&gt; 然后，也许还需要看一下 IntelliJ 热部署的方式，这里不再给出链接。 部署 从很久之前就非常、非常、非常讨厌重装系统，进行各种配置没有太大问题，但时隔一年半载再次配置就无法忍受了。所以，对虚拟化一直很感兴趣，装过无数虚拟机。在看到 docker 后无比兴奋，目前的想法是如何把每个 windows 程序做成类似于此，不再为重装或迁移系统而费脑筋（打包软件 Cameyo 并不好用）。 与上面类似，还是推荐一篇文章： Docker自动部署Apache Tomcat 写的非常好，不过我还是要进行一些更新。 本人使用的是 tomcat8. 1.tomcat-users.xml 中 12&lt;role rolename=&quot;manager-gui&quot;/&gt;&lt;role rolename=&quot;manager-gui&quot;/&gt; 很明显是重复的，根据原文件（tomcat 解压后的 tomcat-users.xml）的说明以及在不进行配置尝试在网页页面进入“Manager App”按钮，并选择取消，会发现有建议的更改方式，具体如下： 123456789101112&lt;?xml version=&apos;1.0&apos; encoding=&apos;utf-8&apos;?&gt;&lt;tomcat-users&gt;&lt;role rolename=&quot;manager-gui&quot;/&gt; &lt;role rolename=&quot;manager-script&quot;/&gt;&lt;role rolename=&quot;manager-jmx&quot;/&gt; &lt;role rolename=&quot;manager-status&quot;/&gt;&lt;user username=&quot;yourusername&quot; password=&quot;yourpassword&quot; roles=&quot;manager-gui,manager-script,manager-jmx,manager-status&quot;/&gt; &lt;/tomcat-users&gt; 2.原 Dockerfile 中 ADD settings.xml /usr/local/tomcat/conf/ 并不太清楚是为何，所以，不再添加。 3.想要远程部署，需要添加 manager.xml 到指定目录，反映到 Dockerfile 就是添加： ADD manager.xml /usr/local/tomcat/conf/Catalina/localhost/ 具体的，可以参见我写好的一个 dockerfile。 远程部署与持续集成 Tomcat 按前面 docker 容器构建即可。 想要持续集成与远程部署，这里选择 Jenkins，因为 VPS 有限，所以部署在本地（官方提供了 docker）。 参见教程： jenkins+maven+svn构建项目，及远程部署war包到tomcat上 以及 「Jenkins+Git+Maven+Shell+Tomcat持续集成」经典教程。 事实证明很爽，毕竟是只需将代码提交到 github 便可以自动编译部署。我的一个 demo。 使用 jenkins docker 镜像的一些说明 [Update 20160916] 对于 docker 镜像（这里特指 jenkins:2.7.4-alpine，其余的应该都是差不多，毕竟诸多构建工具的版本那么多，不会给定死的），还需要选择安装 maven（官方的镜像中安装了 git 以及 openjdk，但是 Maven、Gradle、Ant 均未安装），其实“安装”很简单，我们只是提供一个“名字”罢了，以安装 Maven 为例： &gt;（主页面）系统管理 -&gt; Global Tool Configuration -&gt; Maven 安装 -&gt; 新增 Maven 会看到 “Name” 栏为空，我们只需给出一个名字即可，比如说 “mvn3.3.9”（此时 Apache 提供的是 3.3.9 版本），下面的自动安装的来源看着选即可。 但是，这样还不行，会发现源代码更新后出现： &gt;ERROR: Failed to parse POMs java.io.IOException: Cannot run program "/bin/java" (in directory "/var/jenkins_home/workspace/***"): error=2, No such file or directory 直接 sudo docker run -it jenkins:2.7.4-alpine /bin/sh 进去后执行 /bin/java 出现 /bin/sh: /bin/java: not found，所以还需要对 java 进行配置。仍旧是上面修改 maven 的页面，把 JDK 对应的 JAVA_HOME 改为 /usr/lib/jvm/java-1.8-openjdk（见 openjdk:8-jdk-alpine）内的设置。 总结一下，使用 Jenkins 总归是要配置 JDK，Maven，Git 等的，不管是在本地还是 Docker 上。 Deploy war/ear to a container 的小技巧 其中有一个要填的内容 Context path，假如说在本地运行时（Intellij）的 URL 为 http://localhost:9000/youareawesome 那么在 Context path 填任何非空的东西，比如说 aa 或者 \aa 那么在调用时只能是 http://yourRemoteIp:9000/aa/youareawesome 会注意到在 URL 中生硬的多出了 aa，那么什么都不填呢？会发现 aa 得换成你的工程名才能访问，比如说你和我一样使用的上面说到的 webapp 的模板，那么得访问 http://yourRemoteIp:9000/webapp/youareawesome 才好。这些我们都不喜欢，其实只要填入 \ 即可，想必这也容易理解。]]></content>
      <categories>
        <category>Configuration</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>Java</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
</search>
